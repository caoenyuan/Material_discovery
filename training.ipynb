{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76a8afe9-1f35-48de-8384-6063d350cbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "340eccc8-60ac-4b8c-9807-dcbbc86185e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d5177f54ff74fd6bb26264fb4746c1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             #load_in_8bit=True,\n",
    "                                             torch_dtype=torch.float16,\n",
    "                                             device_map=\"auto\"\n",
    "                                            )\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc9148cc-49d4-4e74-bc10-1a8ef0788108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8334c99e75c4e21b9c1a9b645774910",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             torch_dtype=torch.bfloat16,\n",
    "                                             device_map=\"auto\"\n",
    "                                            )\n",
    "tokenizer = AutoTokenizer.from_pretrained('results/checkpoint-20100')\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model.load_adapter('results/checkpoint-20100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5829a40-678e-4e00-9521-b4eb6dd81b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['title', 'abstract', 'doi', 'publicationDate'],\n",
      "    num_rows: 165071\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "data = load_dataset(\"enyuan/Abstracts\")\n",
    "data_train = data[\"train\"]\n",
    "print(data_train)\n",
    "\n",
    "#custom_data = load_dataset('json', data_files='data_eval.json')\n",
    "#data_val = custom_data['train']\n",
    "\n",
    "with open('materials.txt', 'r') as file:\n",
    "    word_list = file.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5266184a-3a3a-485f-a4af-f8d2a246407b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset('csv', data_files=\"gdc.csv\")\n",
    "data_train = concatenate_datasets([data_train, data[\"train\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9660a32-e7fd-43b7-ab6b-cc1286c95508",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = {\n",
    "    'title': word_list,\n",
    "    'abstract': [s.replace('_', '') for s in word_list],\n",
    "    'doi': ['material'] * len(word_list),  # 假设新数据集中没有doi信息\n",
    "    'publicationDate': [None] * len(word_list)  # 假设新数据集中没有publicationDate信息\n",
    "}\n",
    "new_dataset = Dataset.from_dict(new_data)\n",
    "\n",
    "data_train = concatenate_datasets([data_train, new_dataset])\n",
    "\n",
    "new_data = {\n",
    "    'title': [s.replace('_', '') for s in word_list],\n",
    "    'abstract': word_list,\n",
    "    'doi': ['material'] * len(word_list),  # 假设新数据集中没有doi信息\n",
    "    'publicationDate': [None] * len(word_list)  # 假设新数据集中没有publicationDate信息\n",
    "}\n",
    "new_dataset = Dataset.from_dict(new_data)\n",
    "\n",
    "data_train = concatenate_datasets([data_train, new_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d82ff71a-8efd-407b-9017-cc0fee007c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_val = data_train.select(range(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b588386b-7f22-4463-bd29-81d1cfc26949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] <<SYS>> You are a helpful scientific assistant, answer the abstract of the below paper.<</SYS>>\n",
      " Experimental analysis on drilling of super duplex stainless steel 2507 (SDSS 2507) using cryogenic LCO_2 and MQL process [/INST]\n",
      " Abstract: Environmental-friendly liquid carbon dioxide (LCO_2) and biodegradable coconut oil–based minimum quantity lubrication (MQL) technique play a significant role in green machining compared to conventionally polluting cutting fluids. In this work, analysis of the drilling performance was made for super duplex stainless steel (SDSS) which finds use in numerous industrial applications in marine, petrochemical, and oil industries. Input parameters chosen were the cutting velocity of 60 m/min, feed rate of 0.03, 0.05, 0.07 mm/rev, and varying environmental conditions such as LCO_2, MQL, and flood coolant. Comparison between output parameters and analysis was made in all the environmental conditions based on cutting temperature (T), surface topography, surface roughness (R_a), tool wear, and chip morphology. The application of LCO_2-based drilling process resulted in the dwindling of the cutting temperature (T), improved surface finish, and better chip breakability in comparison to other drilling conditions. Using a scanning electron microscope (SEM), the LCO_2 condition displays the least amount of flank and crater wear. </s> \n"
     ]
    }
   ],
   "source": [
    "def generate_prompt(type_, prompt, output=None, eos_token=\"</s>\"):\n",
    "    begin = \"<s>[INST]\"\n",
    "    if type_ == 'material':\n",
    "        instruction = \"<<SYS>> You are a helpful scientific assistant, answer the composition of the following material.<</SYS>>\\n\"\n",
    "        prompt = f\"{prompt} is [/INST]\\n\"\n",
    "        output = f\"{output + ' ' + eos_token if output else ''} \"\n",
    "    elif type_ == 'gilbert':\n",
    "        instruction = \"<<SYS>> You are a helpful scientific assistant, answer the question below. Do not answer with anything other than a value.<</SYS>>\\n\"\n",
    "        prompt = f\"The Gilbert damping constant of {prompt}[/INST]\\n\"\n",
    "        output = f\"{str(output) + ' ' + eos_token if output else ''} \"\n",
    "    elif type_ == 'summary':\n",
    "        instruction = \"<<SYS>> You are a helpful scientific assistant, answer the summary of the below paper.<</SYS>>\\n\"\n",
    "        prompt = f\"The Gilbert damping constant of {prompt}[/INST]\\n\"\n",
    "        output = f\"{str(output) + ' ' + eos_token if output else ''} \"\n",
    "    else:\n",
    "        instruction = \"<<SYS>> You are a helpful scientific assistant, answer the abstract of the below paper.<</SYS>>\\n\"\n",
    "        prompt = f\"{prompt} [/INST]\\n\"\n",
    "        output = f\"Abstract: {output + ' ' + eos_token if output else ''} \"\n",
    "    #end = \"[/INST]\\n\"\n",
    "    prompt = (\" \").join([begin, instruction, input, output])\n",
    "    return prompt\n",
    "\n",
    "print(generate_prompt(data_train[1][\"doi\"], data_train[1][\"title\"], data_train[1][\"abstract\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c00bdbd1-cbb4-40cf-b95e-f5722de31bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INST] <<SYS>> You are a helpful scientific assistant, answer the composition of the following material.<</SYS>>\n",
      " NiFeAlO4 is [/INST]\n",
      "   Sure! The chemical formula for NiFeAlO4 is Nickel(II) iron(III) aluminum oxide.\n"
     ]
    }
   ],
   "source": [
    "input_prompt = generate_prompt(data_train[-1][\"doi\"], data_train[-1][\"title\"])\n",
    "input_tokens = tokenizer(input_prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "with torch.cuda.amp.autocast():\n",
    "  generation_output = model.generate(\n",
    "      input_ids=input_tokens,\n",
    "      max_new_tokens=128,\n",
    "      do_sample=True,\n",
    "      top_k=10,\n",
    "      top_p=0.9,\n",
    "      temperature=0.3,\n",
    "      repetition_penalty=1.15,\n",
    "      num_return_sequences=1,\n",
    "      eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "op = tokenizer.decode(generation_output[0], skip_special_tokens=True)\n",
    "print(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a4ce5ac-170a-42c2-8b05-b204b6bf9f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "        r=128,\n",
    "        lora_alpha=256,\n",
    "        lora_dropout=0.05,\n",
    "        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"],\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6bb07ccc-4f63-44d0-8e25-16ad278ff7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 5552 tokens\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Add new tokens to the tokenizer\n",
    "num_added_toks = tokenizer.add_tokens(word_list)\n",
    "print(f\"Added {num_added_toks} tokens\")\n",
    "\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"<PAD>\"})\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "#model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f119b5df-ac5a-46f8-be81-b12943b162f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Freeze all parameters in the model\n",
    "#for param in model.parameters():\n",
    "#    param.requires_grad = False\n",
    "\n",
    "embeddings = model.get_input_embeddings()\n",
    "\n",
    "# Enable gradient updates for the entire embedding layer\n",
    "# Assuming you might want to fine-tune all embeddings, but here's how to selectively unfreeze\n",
    "embeddings.weight.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8483b140-a7e6-4175-8e01-6e3f13879301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='results',            # 输出目录\n",
    "    num_train_epochs=2,              # 总训练轮数\n",
    "    per_device_train_batch_size=4,   # 训练的batch size\n",
    "    per_device_eval_batch_size=4,    # 验证的batch size\n",
    "    gradient_accumulation_steps=4, \n",
    "    #gradient_checkpointing=True,\n",
    "    #optim = \"paged_adamw_32bit\",\n",
    "    optim = \"adamw_torch\",\n",
    "    bf16=True,\n",
    "    #fp16=True,\n",
    "    warmup_steps=300,                # 预热步数\n",
    "    learning_rate = 1e-4,\n",
    "    max_grad_norm = 0.2,\n",
    "    #max_steps = 50,\n",
    "    #warmup_ratio = 0.03,\n",
    "    #weight_decay=0.01,               # 权重衰减\n",
    "    save_strategy=\"steps\",           # 设置保存策略为\"steps\"\n",
    "    save_steps=300,                  # 每500步保存一次模型\n",
    "    save_total_limit=3,              # 最多保存3个检查点\n",
    "    evaluation_strategy=\"epoch\",     # 设置评估策略为\"steps\"\n",
    "    group_by_length=True,\n",
    "    #eval_steps=10000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8e64ad6-36bd-4b68-b995-bfef056bb7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient checkpointing enabling\n",
    "model.enable_input_require_grads()\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d49402b-1027-4e8b-a777-54b6b03f7966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa1bc8ffa337439ab94a353000a9122e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/187279 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f5b44c3eb81484cad84fbbddeee1b3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='253' max='23410' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  253/23410 14:05 < 21:40:21, 0.30 it/s, Epoch 0.02/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def formatting_func(prompt):\n",
    "  output = []\n",
    "\n",
    "  for a, d, s in zip(prompt[\"doi\"], prompt[\"title\"], prompt[\"abstract\"]):\n",
    "    op = generate_prompt(a, d, s)\n",
    "    output.append(op)\n",
    "\n",
    "  return output\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=data_train,\n",
    "    eval_dataset=data_val,\n",
    "    peft_config=lora_config,\n",
    "    formatting_func=formatting_func,\n",
    "    max_seq_length=512,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args\n",
    ")\n",
    "\n",
    "# We will also pre-process the model by upcasting the layer norms in float 32 for more stable training\n",
    "#for name, module in trainer.model.named_modules():\n",
    "#    if \"norm\" in name:\n",
    "#        module = module.to(torch.float32)\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(f\"{output_dir}/final\")\n",
    "\n",
    "# Step Training Loss Validation Loss\n",
    "# 10 1.848200 1.746341\n",
    "# 20 1.688300 1.696681\n",
    "# 30 1.654500 1.698127\n",
    "# 40 1.579400 1.652010\n",
    "# 50 1.492600 1.701877"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0c637aa-9ed2-40f7-a0fe-d0658babc43b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ef6d36bfaee4d08b043bda05be32f0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = load_dataset('json', data_files='selected_paragraphs.json')\n",
    "data = data['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "016e9af6-1208-4a49-82fd-f778b4c2378d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] <<SYS>> You are a helpful assistant. Read the following text, if it mention the Gilbert damping constant of a certain material, list the corresponding material and its Gilbert damping canstant. Make your answer as short as possible.<</SYS>>\n",
      " ce as ∆ E(φ) given by Eq. (6). Due to this spe-\n",
      "cial property the energy proﬁle ﬂips upside down at\n",
      "u=√ω||ω⊥. For a generic Er(π/2,φ) with minima at\n",
      "φ= 0,πand maxima at φ=±π/2 the nature of equilib-\n",
      "ria will change at diﬀerent current thresholds. This will\n",
      "make the switching diagram more complicated, but will\n",
      "notaﬀectthestabilizationbyrepulsionphenomena. Sim-\n",
      "ilarcomplicationswillbe introducedbyageneric f[(n·s)]\n",
      "angular dependence of the spin transfer strength.\n",
      "In Ref. 13 the known switching diagram for the\n",
      "collinear ( φs= 0) devices [6, 9, 10] were reproduced\n",
      "by equation (3) with Eeff=Er(π/2,φ). The ∆ E\n",
      "term (6) was dropped as being second order in small\n",
      "u. This approximation gives a correct result for the\n",
      "following reason. In a collinear device ( γ/M)Eeff=\n",
      "−[(ω||+u2/ω⊥)/2]cos2φ+const and the current never\n",
      "changes the nature of the equilibrium from a maximum\n",
      "to a minimum. Consequently, dropping ∆ Edoes not af-\n",
      "fect the results. As was already noted in Ref. 13, the ﬁrst\n",
      "order expansion in uis insuﬃcient for the description of\n",
      "a spin-ﬂip transistor, where the full form (6) is required.\n",
      "In summary, we derived a general form of the eﬀec-\n",
      "tive planar equation (3) for a macrospin free layer in\n",
      "the presence of spin transfer torque produced by a ﬁxed\n",
      "spin-polarizerandtime-independent current. Qualitative\n",
      "understanding of the solutions of planar equation is ob-\n",
      "tained by employing the analogy with a one-dimensional\n",
      "mechanical motion of a particle with variable friction co-\n",
      "eﬃcient in an external potential. The resulting predic-\n",
      "tive power is illustrated by the discovery of the stabi-\n",
      "lization by repulsion phenomena in the spin-ﬂip device.\n",
      "Such stabilization relies on the form of the dissipative\n",
      "torquesin the LLG equationand happens onlyfora large\n",
      "enough Gilbert damping constant. The new stable stateand the corresponding precession cycle can be used to\n",
      "engineer novel memory or logic devices, and microwave\n",
      "nano-generators with tunable frequency.\n",
      "To observe the phenomena experimentally, one has to\n",
      "fabricate a device with α > α∗, and initially set it into a\n",
      "parallel or antiparallel state by external magnetic ﬁeld.\n",
      "Thenthecurrentisturnedonandtheﬁeldisswitchedoﬀ.\n",
      "Both states should be stabilized by a moderate current√ω||ω⊥< u < αω ⊥/2, but cannot yet be distinguished\n",
      "by their magnetoresistive signals. The diﬀerence can be\n",
      "observed as the current is increased above the αω⊥/2\n",
      "threshold: the parallel state will remain a stable equilib-\n",
      "rium, while the antiparallel state will transform into a\n",
      "precession cycle and an oscillating component of magne-\n",
      "toresistance will appear.\n",
      "The author wishes to thank C. W. J. Beenakker, G.\n",
      "E. W. Bauer, and Yu. V. Nazarov for illuminating dis-\n",
      "cussions. Research at Leiden University was supported\n",
      "by the Dutch Science Foundation NWO/FOM. Part of\n",
      "this work was performed at KITP Santa Barbara sup-\n",
      "ported by the NSF grant No. PHY99-07949, and at As-\n",
      "pen PhysicsInstitute duringthe Winterprogramof2007.\n",
      "[1] L. Berger, J. Appl. Phys., 49, 2160 (1978); Phys. Rev. B\n",
      "33, 1572 (1986); J. Appl. Phys. 63, 1663 (1988).\n",
      "[2] J. Slonczewski, J. Magn. Magn. Mater. 159, L1 (1996).\n",
      "[3] Ya. B. Bazaliy et al., Phys. Rev. B, 57, R3213 (1998).\n",
      "[4] S. Kaka et al., Nature 437, 389 (2005); M. R. Pufall et\n",
      "al., Phys.Rev. Lett. 97, 087206 (2006); M. L. Schneider\n",
      "et al., Appl. Phys. Lett., 90, 092504 (2007); X. Jiang\n",
      "et al., Phys. Rev. Lett. 97, 217202 (2006); W. Chen et\n",
      "al., Phys. Rev. B, 74, 144408(2006); B. Ozyilmaz et al.,\n",
      "Phys. Rev. Lett., 93, 176604 (2004); I. N. Krivorotov et\n",
      "al.Science, 307, 228 (2005); N. C. Emley et al.Phys.\n",
      "Rev. Lett., 96, 247204 (2006); J. C. Sankey, et al., Phys.\n",
      "Rev. Lett., 96, 227601 (2006).\n",
      "[5] G. Beach et al., Phys. Rev. Lett., 97, 057203 (2006); Na-\n",
      "ture Materials, 4, 741 (2005); M. Klaui et al., Phys. Rev.\n",
      "Lett.,95, 026601 (2005); M. Laufenberg et al., Phys.\n",
      "Rev. Lett., 97, 046602 (2006); L. Thomas et al., Science,\n",
      "315, 1553 (2007); M. Hayashi et al., Phys. Rev.Lett., 98,\n",
      "037204 (2007); Nature Physics, 3, 21 (2007); Phys. Rev.\n",
      "Lett.,97, 207205 (2006); M. Yamanouchi et al.Nature,\n",
      "428, 539 (2004); Phys. Rev. Lett., 96, 096601 (2006).\n",
      "[6] Ya. B. Bazaliy et al., Phys. Rev. B, 69, 094421 (2004).\n",
      "[7] J. Z. Sun, Phys. Rev. B 62, 570 (2000).\n",
      "[8] J. A. Katine et al., Phys. Rev. Lett., 84, 3149 (2000).\n",
      "[9] S. I. Kiselev et al., Nature, 425, 380 (2003).\n",
      "[10] J. Xiao et al., Phys. Rev. B, 72, 014446 (2005)\n",
      "[11] A. Brataas et al., Phys. Rep., 427, 157 (2006).\n",
      "[12] C. Garcia-Cervera et al., J. Appl. Phys., 90, 370 (2001).\n",
      "[13] Ya. B. Bazaliy et al., arXiv:0705.0406v1 (2007), to be\n",
      "published in J. Nanoscience and Nanotechnology.\n",
      "[14] A. Brataas et al., Phys. Rev. Lett. 84, 2481 (2000);\n",
      "X. Wang et al., Japan. J. Appl. Phys., 45, 3863 (2006).\n",
      "[15] X. Wang et al., Phys. Rev. B, 73, 054436 (2006). [/INST]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_prompt(content):\n",
    "    begin = \"<s>[INST]\"\n",
    "    #syst = \"<<SYS>> You are a helpful assistant, always answer as helpfully as possible.\\n If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.<</SYS>>\\n\"\n",
    "    #inst = \"Read the following text. Does it mention the Gilbert damping constant of a certain material? If so, list the corresponding material and its Gilbert damping canstant.\\n\" + content\n",
    "    syst = \"<<SYS>> You are a helpful assistant. Read the following text, if it mention the Gilbert damping constant of a certain material, list the corresponding material and its Gilbert damping canstant. Make your answer as short as possible.<</SYS>>\\n\"\n",
    "    inst = content\n",
    "    end = \"[/INST]\\n\"\n",
    "    prompt = (\" \").join([begin, syst, inst, end])\n",
    "    return prompt\n",
    "\n",
    "print(generate_prompt(data[0]['content']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be1eda10-0875-4b4b-9b45-0a691513aa38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The Gilbert damping constant of materials mentioned in the text are:\n",
      "\n",
      "* Aluminum: α = 0.027 (Ref. [1])\n",
      "* Copper: α = 0.022 (Ref. [2])\n",
      "* Magnetic semiconductors: α = 0.01-0.05 (Ref. [3])\n",
      "* Iron: α = 0.04 (Ref. [4])\n",
      "* Nickel: α = 0.03 (Ref. [5])\n",
      "* Cobalt: α = 0.\n",
      "\n",
      "The Gilbert damping constant mentioned in the article is α = 0.05 meV/rad for the material NiFe.\n",
      "\n",
      "The material mentioned in the text is a metallic ferromagnet, and the Gilbert damping constant is denoted by αG.\n",
      "\n",
      "The Gilbert damping constant mentioned in the article is listed below:\n",
      "\n",
      "* Ferromagnetic layer (F): γ_F = 4.3 × 10^4 rad/s\n",
      "* Non-magnetic layer (N): γ_N = 1.3 × 10^3 rad/s\n",
      "\n",
      "Note that these values are specific to the particular material system being studied and may not be applicable to other materials.\n",
      "\n",
      "In this text, the Gilbert damping constant is mentioned for the non-collinear F/N/F trilayer system with ρ = π/2. The enhancement of the Gilbert damping constant is given by α' = gL μB g↑↓ 8π MdF1 S / (1 - ν cot θ cos ψ sin ωt), where ν = (g↑↓ - g*) / (g↑↓ + g*) is the dimensionless parameter introduced in Ref. [17].\n",
      "\n",
      "The material mentioned in the passage is Py/Cu, and its Gilbert damping constant is given by gL = 2.1.\n",
      "\n",
      "The material mentioned in the article is a ferromagnetic material, and the Gilbert damping constant α is listed as 0.16 rad/s in the article.\n",
      "\n",
      "The Gilbert damping constant is mentioned in the following materials:\n",
      "\n",
      "* Magnetic insulators [8, 9, 10]\n",
      "* Metallic systems [13, 47, 49, 50]\n",
      "* Nanowires [32, 33, 34, 35, 36, 37, 38, 39]\n",
      "* Ferromagnetic multilayers [23, 24, 25, 26, 27, 28, 29,\n",
      "\n",
      "Based on the given text, the Gilbert damping constant of the following materials can be listed as follows:\n",
      "\n",
      "* Fe layer: G = 239 MHz\n",
      "* Fe 0.63V0.37 alloy: G = 145 MHz\n",
      "\n",
      "Note that these values are estimated based on the analysis conducted in the text and may not be exact values.\n",
      "\n",
      "The material mentioned in the text is magnetic multilayers, specifically those with a pinned and a free layer separated by a non-magnetic spacer layer. The Gilbert damping constant of this material is denoted as α.\n",
      "\n",
      "According to the text, the value of α is related to the value of the coefficiency β in the following way:\n",
      "\n",
      "α = β\n",
      "\n",
      "This means that the Gilbert damping constant of magnetic multilayers is equal to the non-adiabatic torque coefficient, which is a measure of the spin-transfer torque-\n",
      "\n",
      "The Gilbert damping constant is mentioned in the text as follows:\n",
      "\n",
      "* \"However, if /angbracketleftM/angbracketright(∇·v)/negationslash= 0, the steady-state solution may break the Galilean invariance. The situation /angbracketleftM/angbracketright(∇·v)/negationslash= 0 can be realized, for example, in magnetic semiconductors [23, 24], where the spin carrier density is spatially inhomogeneous, i.e.,∇ρ/negations\n",
      "\n",
      "The Gilbert damping constant of the material mentioned in the text is α = 0.01.\n",
      "\n",
      "The material mentioned in the article is Ho metal, which has a Gilbert damping constant of α = 0.025 meV/rad and β = 0.05 meV/rad.\n",
      "\n",
      "The Gilbert damping constant of the material is not explicitly mentioned in the text, but it can be inferred based on the context. The Gilbert damping constant, α, is a measure of the energy loss per cycle of magnetization precession in a magnetic material. In the text, it is mentioned that the energy dissipation rate of a DW is related to the Gilbert damping constant and gyromagnetic ratio, α, and γ, respectively.\n",
      "\n",
      "Assuming that the material is a ferromagnetic material with a non-zero Gilbert damping constant, we can estimate the value of\n",
      "\n",
      "The material mentioned in the article is (Ga,Mn)As, and its Gilbert damping constant is listed as α = 0.01 and β = 1.\n",
      "\n",
      "According to the text, the Gilbert damping constant of (Ga,Mn)As is:\n",
      "\n",
      "αw = 5 × 10^-3 for λw/l = 0.5\n",
      "αw = 6 × 10^-3 for γ2 = 2.5\n",
      "\n",
      "Note that these values are given for a specific set of parameters (λw = 40nm, l = 23nm, and GaAs material), and may not be applicable to other materials or conditions.\n",
      "\n",
      "The Gilbert damping constant of (Ga,Mn)As is approximately αw ≈ 0.01.\n",
      "\n",
      "The material mentioned in the article is NiCo multilayer film with perpendicular magnetic anisotropy. The Gilbert damping constant for this material is estimated to be α ≈ 0.04.\n",
      "\n",
      "The Gilbert damping constant mentioned in the text is γ. Its value is given as:\n",
      "\n",
      "γ = ωΔ / (2πk_B T)\n",
      "\n",
      "where ωΔ is the spectral linewidth predicted by equation (5), and k_B is Boltzmann's constant.\n",
      "\n",
      "* Gilbert damping constant: The material mentioned in the passage is \"a ferromagnet\" (not specified), and the Gilbert damping constant is given by α/s0 ≈ 1.\n",
      "\n",
      "The Gilbert damping constant in the text is α = 0.1.\n",
      "\n",
      "In the given text, the Gilbert damping constant of MnSi is mentioned. According to the text, the Gilbert damping constant of MnSi is approximately 0.1.\n",
      "\n",
      "The Gilbert damping constant is mentioned in the paper as a material property, and its value depends on the specific material being studied. Here are some examples of materials where the Gilbert damping constant has been measured or calculated:\n",
      "\n",
      "* In Ni80Fe20, the Gilbert damping constant is around 0.1 ps^(-1) at room temperature. (Ref: K. S. Lyo et al., Phys. Rev. B 70, 165303 (2004))\n",
      "* For CoFeB/Cu(001) multil\n",
      "\n",
      "The material mentioned in the article is MgO, and its Gilbert damping constant is given as α = 0.025 meV/rad.\n",
      "\n",
      "The material mentioned in the paper is Tohoku University's Applied Physics Department's magnetic tunnel junction (MTJ). The Gilbert damping constant of this material is not explicitly stated in the paper, but it can be inferred from the calculations presented in the paper.\n",
      "\n",
      "Using the Landau-Lifshitz-Gilbert (LLG) equation and the tight-binding model, the authors of the paper calculated the charge and spin currents passing through an MTJ in the presence of a voltage across the barrier and the dynamical magnetization in the free layer. They found that the\n",
      "\n",
      "The Gilbert damping constant of several materials can be listed as follows:\n",
      "\n",
      "* Silicon (Si): 0.025 meV\n",
      "* Germanium (Ge): 0.015 meV\n",
      "* Gallium arsenide (GaAs): 0.035 meV\n",
      "* Indium antimonide (InSb): 0.025 meV\n",
      "* Lead telluride (PbTe): 0.045 meV\n",
      "\n",
      "These values are based on experimental measurements and theoretical calculations, and they represent the typical values of\n",
      "\n",
      "The Gilbert damping constant of the material mentioned in the text is α = 0.001.\n",
      "\n",
      "\n",
      "The Gilbert damping constant mentioned in the article is:\n",
      "\n",
      "α = 0.017 ps^(-1)\n",
      "\n",
      "This value is obtained from the expression:\n",
      "\n",
      "α = 2(S F B eM PJg) / (m^2 x γ x μB x e)\n",
      "\n",
      "where SFB is the spin Hall angle, P is the spin polarization in the FM, Jg is the gyromagnetic ratio, μB is the Bohr magneton, e is the electron charge, and m is the unit vector along the magnetization.\n",
      "\n",
      "The material mentioned in the text is EuO, and its Gilbert damping constant is approximately 3 x 10^-3.\n",
      "\n",
      "The Gilbert damping constant of the material mentioned in the text is EuO, and its value is approximately 3 x 10^-3.\n",
      "\n",
      "In this article, the authors calculate the Gilbert damping parameter α for various materials using the linear response formalism. They consider both intrinsic and extrinsic sources of damping, including electron magnon scattering, interface relaxation mechanisms, and spin pumping. The materials studied include elemental magnetic systems, ordered magnetic compounds, and disordered magnetic alloys. The authors find good agreement between their calculated values of α and experimental data.\n",
      "\n",
      "The materials mentioned in the article where the Gilbert damping constant was calculated are:\n",
      "\n",
      "* Iron (Fe)\n",
      "* Cobalt (Co)\n",
      "*\n",
      "\n",
      "The Gilbert damping constant of the material is not explicitly mentioned in the given text, but based on the context, it can be inferred that the material being discussed is a ferromagnetic material. The Gilbert damping constant, α, is a material parameter that describes the intrinsic damping mechanism in ferromagnetic materials due to the thermal fluctuations of the magnetic moments.\n",
      "\n",
      "In the text, the authors derive the current dependence of the oscillation frequency of a spin torque oscillator consisting of a perpendicularly magnetized free layer and an in-plane magnetized pinned layer. They\n",
      "\n",
      "* Material: Nickel manganese sulfide (NiMnSb)\n",
      "* Gilbert damping constant: Not mentioned in the passage.\n",
      "\n",
      "The Gilbert damping constant of the material mentioned in the text is not explicitly stated, but it can be calculated based on the parameters provided.\n",
      "\n",
      "The Gilbert damping constant (γ) is related to the magnetization dynamics described by the Landau-Lifshitz-Gilbert (LLG) equation. In the text, the LLG equation is given in equation (1), which includes the Gilbert damping constant (γ) as one of the parameters.\n",
      "\n",
      "Using the parameters given in the text, we can estimate the value of γ for the material under consideration. From equation (1),\n",
      "\n",
      "The Gilbert damping constant (α) for the materials mentioned in the text are:\n",
      "\n",
      "* For (Ga,Mn)As: α = (2.5 ± 0.2) meVnm2\n",
      "* For (Ga,Mn)(As,P): α = (1.9 ± 0.2) meVnm2\n",
      "\n",
      "These values are obtained by fitting the measured dynamical MO signals using the LLG equation.\n",
      "\n",
      "The Gilbert damping constant of the material mentioned in the text is the NiFe layer, which is given by:\n",
      "\n",
      "α' = γ0 / (planckover2pi1 ˜g↑↓ r(F1)(1 - c))\n",
      "\n",
      "where γ0 is the gyromagnetic ratio, ˜g↑↓ r(F1) is the renormalized conductance of the F1/N interface, and c is a dimensionless coefficient depending on the details of the system.\n",
      "\n",
      "In the text, the value of α' is calculated to be approximately 0.01\n",
      "\n",
      "The Gilbert damping constant of the material is not explicitly mentioned in the text, but based on the values provided in the reference [46], we can estimate the Gilbert damping constant for a typical ferromagnetic metal such as Co or Ni.\n",
      "\n",
      "For Co, the Gilbert damping constant is typically around 0.01 - 0.02 [46].\n",
      "\n",
      "For Ni, the Gilbert damping constant is typically around 0.001 - 0.005 [46].\n",
      "\n",
      "So, based on these values, the Gilbert damping constant for\n",
      "\n",
      "The material mentioned in the article is Yttrium Iron Garnet (YIG). The Gilbert damping constant of YIG is estimated to be α = (8.79 ± 0.73) × 10^-4.\n",
      "\n",
      "Based on the given text, the Gilbert damping constant of several materials can be listed as follows:\n",
      "\n",
      "* Aluminum: α = 0.025 rad/s\n",
      "* Copper: α = 0.015 rad/s\n",
      "* Iron: α = 0.04 rad/s\n",
      "* Nickel: α = 0.03 rad/s\n",
      "\n",
      "Note that these values are approximate and may vary depending on the specific material and experimental conditions.\n",
      "\n",
      "The material mentioned in the passage is Co (Cobalt).\n",
      "\n",
      "The Gilbert damping constant G is listed as follows:\n",
      "\n",
      "* For unsupported ultrathin Co films: G = 0 as \u0011 approaches 0.\n",
      "* For a fixed value of \u0018 (spin-orbit coupling strength): G approaches finite values as \u0011 approaches 0+.\n",
      "\n",
      "Note that the value of G depends on the specific material and the amount of spin-orbit coupling present.\n",
      "\n",
      "The Gilbert damping constant for the material mentioned in the text is:\n",
      "\n",
      "* For Fe: λFe = 0.628 meV\n",
      "* For Ni: λNi = 0.730 meV\n",
      "\n",
      "These values are obtained from the ab initio calculated exchange constants (Jij) shown in Figure 1.\n",
      "\n",
      "The Gilbert damping constant of the material is not explicitly mentioned in the given text, but it can be inferred from the context. The Gilbert damping constant, denoted by α, is a parameter in the LLG equation that describes the damping of magnetic moments due to magnetic fields. In the present study, the authors investigate the effect of notches on domain wall (DW) propagation in magnetic nanowires, and they find that the depinning current density of a DW trapped in an notch is smaller than the intrinsic threshold current density in the absence of non-adiabatic spin\n",
      "\n",
      "The Gilbert damping constant of the material mentioned in the text is:\n",
      "\n",
      "* For sample 1: 1.0 × 10^-3\n",
      "* For sample 2: 1.5 × 10^-2\n",
      "* For sample 3: 1.0 × 10^-3\n",
      "\n",
      "These values are obtained from the plot of Gilbert damping against the vertical lattice constant in Figure 3(b) of the text.\n",
      "\n",
      "The material mentioned in the article is Mn 3Ge, and its Gilbert damping constant is approximately 0.03.\n",
      "\n",
      "Material: Ultrathin CoFeB layers and Co/Ni/Co trilayers\n",
      "Gilbert damping constant: Not specified in the given text.\n",
      "\n",
      "The material mentioned in the passage is Ir/Co. The Gilbert damping constant of Ir/Co is 1.36 mJ/m2.\n",
      "\n",
      "The Gilbert damping constant of Permalloy, a material mentioned in the text, is not explicitly stated. However, based on the context, it can be inferred that the authors are referring to the Gilbert damping constant of Permalloy, which is typically denoted as αPG.\n",
      "\n",
      "According to the literature, the Gilbert damping constant of Permalloy is around 0.03-0.04 (Ref: J. M. D. Coey, \"Ferromagnetism: A Study of the Magnetic Properties of Ferromagnetic Materials,\" Oxford University Press, \n",
      "\n",
      "The Gilbert damping constant for Permalloy is α = 0.078 rad/s.\n",
      "\n",
      "The material mentioned in the text is La. The Gilbert damping constant for La is not explicitly given in the text, but it can be calculated using first-principles electronic structure calculations or other methods. Some recent ab initio calculations have predicted values of the Gilbert damping constant for La within a range of a factor of two to three compared to experimental values [22-24, 26-28]. However, there is still a need to improve the fundamental understanding of the origin of spin-moment relaxation in La and other materials.\n",
      "\n",
      "* Material: Galfenol (GdFeCo alloy)\n",
      "* Gilbert damping constant: χ = 100 rad/Oe\n",
      "* Spin Hall angle: θ = 0.01\n",
      "\n",
      "Note: These values are typical for galfenol, but may vary depending on the specific sample and experimental conditions.\n",
      "\n",
      "The Gilbert damping constant of the material Ba2Zn2Fe12O22 is not explicitly mentioned in the given text. However, based on the information provided, we can calculate the Gilbert damping constant using the formula:\n",
      "\n",
      "α = (2π / γ) \\* (k_B / h)\n",
      "\n",
      "where:\n",
      "\n",
      "* α is the Gilbert damping constant\n",
      "* γ is the gyromagnetic ratio\n",
      "* k_B is Boltzmann's constant\n",
      "* h is Planck's constant\n",
      "\n",
      "From the text, we know that the magnetic moment\n",
      "\n",
      "Based on the given text, the Gilbert damping constant of the materials mentioned are:\n",
      "\n",
      "* LP573K: 𝛼𝛼𝑟𝑟𝑚𝑚𝑟𝑟 ≈ 2.36 ±0.10 x 10^-3\n",
      "* LP673K: 𝛼𝛼𝑟𝑟𝑚𝑚𝑟𝑟 ≈ 1.57\n",
      "\n",
      "The material mentioned in the article is GdIG, and its Gilbert damping constant is not explicitly mentioned in the provided text. However, we can estimate the Gilbert damping constant based on the given information.\n",
      "\n",
      "The Gilbert damping constant (or the damping parameter) is defined as:\n",
      "\n",
      "$$ \\Gamma = \\frac{1}{2} \\alpha \\left( \\frac{B_0}{B_0'} \\right)^2 $$\n",
      "\n",
      "where $\\alpha$ is the gyromagnetic ratio, $B_0$ is the applied magnetic field, $B_0'$ is\n",
      "\n",
      "The materials mentioned in the passage are:\n",
      "\n",
      "* Magnetic insulator with Gilbert damping\n",
      "* Disordered magnetic insulator\n",
      "\n",
      "The corresponding Gilbert damping constants are:\n",
      "\n",
      "* Without Gilbert damping: 0\n",
      "* With disorder only: 0.0069\n",
      "* With both disorder and Gilbert damping: 0.0069 or 0.0015 (depending on the specific combination of disorder and Gilbert damping)\n",
      "\n",
      "Based on the text, the Gilbert damping constant of the material mentioned is:\n",
      "\n",
      "* Material: Gadolinium (Gd)\n",
      "* Gilbert damping constant: 0.0016 rad/s\n",
      "\n",
      "The Gilbert damping constant of the material studied in the article is not explicitly mentioned, but it can be inferred from the discussion of the dependence of velocity on the Gilbert damping constant α. The authors find that v|| (the velocity of the skyrmion/antiskyrmion along the gradient direction) increases linearly with α, while v (the velocity of the skyrmion/antiskyrmion perpendicular to the gradient direction) is almost invariant with α. This suggests that the Gilbert damping constant is responsible for the decrease in v|| with increasing α.\n",
      "\n",
      "Based on\n",
      "\n",
      "\n",
      "Here are the materials and their corresponding Gilbert damping constants mentioned in the text:\n",
      "\n",
      "1. Frustrated magnets - Not specified\n",
      "2. Chiral magnets - Not specified\n",
      "3. Skyrmion/antiskyrmion - Not specified\n",
      "4. Material 1 - Not specified (mentioned in reference 15)\n",
      "5. Material 2 - Not specified (mentioned in reference 2)\n",
      "6. Material 3 - Not specified (mentioned in reference 3)\n",
      "7. Material 4 - Not specified (mentioned in reference 4)\n",
      "8. Material\n",
      "\n",
      "The materials mentioned in the text are:\n",
      "\n",
      "* Ni\n",
      "* Co\n",
      "* Pt\n",
      "* Py\n",
      "* Pd\n",
      "\n",
      "And their corresponding Gilbert damping constants are:\n",
      "\n",
      "* Ni: α = 0.016 cm/s\n",
      "* Co: α = 0.025 cm/s\n",
      "* Pt: α = 0.005 cm/s\n",
      "* Py: α = 0.012 cm/s\n",
      "* Pd: α = 0.01 cm/s\n",
      "\n",
      "Note that these values are extracted\n",
      "\n",
      "The Gilbert damping constant mentioned in the text is α.\n",
      "\n",
      "The Gilbert damping constant of the material mentioned in the text is Yttrium Iron Garnet (YIG). According to the text, the Gilbert damping constant of YIG is α = 2.6 x 10^-4, which is one of the lowest values reported for any material.\n",
      "\n",
      "The Gilbert damping constant of the material mentioned in the text is:\n",
      "\n",
      "YIG (Yttrium Iron Garnet): α = 1 × 10^-5\n",
      "\n",
      "Note: The value of α is mentioned in the text as \"ultra-low Gilbert damping constant (α < 1×10-5)\" for the YIG films.\n",
      "\n",
      "The Gilbert damping constant of the YIG (40 nm) /SGGG films was found to be α = 3.8 × 10^-5.\n",
      "\n",
      "The Gilbert damping constant mentioned in the paper is:\n",
      "\n",
      "* For Permalloy (PM): α = 0.027 ps^(-1) [28]\n",
      "\n",
      "Note that the value of α depends on the specific material and can be different from the one listed above.\n",
      "\n",
      "\n",
      "* Material: Py\n",
      "* Gilbert damping constant: α = 1.83 × 10^7 Hz/Oe\n",
      "\n",
      "The Gilbert damping constant (α) of NbN is not explicitly mentioned in the given text. However, the authors do mention that they have extracted the Gilbert damping constant (α) of NbN using spin pumping techniques. They also mention that the extracted α values for different NbN thicknesses do not show any significant modification of the YIG magnetic anisotropy due to the presence of NbN.\n",
      "\n",
      "Therefore, based on the information provided in the text, the Gilbert damping constant of NbN can be estimated to be around 0.1-1\n",
      "\n",
      "The Gilbert damping constant of the materials mentioned in the article are:\n",
      "\n",
      "* YIG (yttrium iron garnet): α = (5.4 ± 0.2) × 10^-4\n",
      "* NbN (niobium nitride): 𝔼𝑟↑↓ = 10 ± 2 nm-2, 𝑙𝑠𝑑 = 14 ± 3 nm\n",
      "\n",
      "Note that these values are extracted from the data presented in the article and are subject to the limitations\n",
      "\n",
      "The materials and their corresponding Gilbert damping constants mentioned in the text are:\n",
      "\n",
      "* CMS: \u000b",
      "001= (1:5\u00060:1)\u000210\u00003\n",
      "* CMA: \u000b",
      "001= (1:8\u00060:2)\u000210\u00003\n",
      "* CFA: \u000b",
      "001= 3\u000210\u00004\n",
      "\n",
      "Note that the values of \u000b",
      "001for CMS and CMA are taken from the text, while the value for CFA is an extrapolation based on the trend of the data\n",
      "\n",
      "The material mentioned in the passage is Graphene, and its Gilbert damping constant is listed as δαG = 2πgvS|J0|2/integraldisplay dε/parenleft biggg -∂f(ε)∂ε/parenright bigg D+(ε)D- (ε)/parenright bigg, where gv = 2 denotes the valley degree of freedom, S is the spin quantum number, and J0 is the exchange interaction strength.\n",
      "\n",
      "The value of δαG for Graphene is not explicitly given in the passage, but it\n",
      "\n",
      "The Gilbert damping constant of the material is given by:\n",
      "\n",
      "$$\\frac{4\\alpha_{opt}}{\\alpha_{B}} = \\frac{7.11\\times 10^{-5} \\text{W}}{333 \\times 10^{-9} \\text{T}} = 21.3 \\times 10^{-6} \\text{T}$$\n",
      "\n",
      "Note that the value of $\\alpha_{opt}$ depends on the specific parameters of the system, such as the laser wavelength, intensity, and detuning.\n",
      "\n",
      "The Gilbert damping constant of the material mentioned in the text is:\n",
      "\n",
      "* W: γ_W = 287 ± 19 μΩ.cm\n",
      "* CoFeB: γ_CoFeB = 139 ± 16 μΩ.cm\n",
      "\n",
      "Note that these values are estimated based on the resistivity measurements and AFM characterization, and may not be directly related to the Gilbert damping constant in the context of magnetic damping.\n",
      "\n",
      "The Gilbert damping constant of the material mentioned in the article is:\n",
      "\n",
      "* Yttrium iron garnet (YIG): α = 2.3 × 10^-4\n",
      "\n",
      "Note that this value is specific to YIG and may not be applicable to other materials.\n",
      "\n",
      "The material mentioned in the article is Cr-PBA (Chromium-phosphorus-boron-arsenic) and its Gilbert damping constant is listed as αeff = 0.0649 ± 0.0031 rad/s.\n",
      "\n",
      "The material mentioned in the article is Cr-PBA (Chromium-Polybenzimidazole). The Gilbert damping constant of Cr-PBA is approximately 7.5 x 10^-4.\n",
      "\n",
      "The material mentioned in the article is Ni0.5Zn0.5Fe2O4. The Gilbert damping constant of this material is α = 6.52 × 10^-2 at a frequency of 2.35 × 10^9 Hz (corresponding to an external magnetic field of 7.02 kOe).\n",
      "\n",
      "The material mentioned in the article is Ni0.5Zn0.5Fe2O4. The Gilbert damping constant (α) of this material is not explicitly mentioned in the article, but it can be calculated using the formula given in the article:\n",
      "\n",
      "α = (2.59 Hdc + 6.25) / (fr^2)\n",
      "\n",
      "where Hdc is the demagnetizing field at the critical field (fr) and α is the Gilbert damping constant.\n",
      "\n",
      "Note that the value of α depends on the specific preparation method and conditions of the Ni0.\n",
      "\n",
      "The Gilbert damping constant of the materials mentioned in the article are:\n",
      "\n",
      "* Co2Fe0.4Mn0.6Si: Gilbert damping constant = 0.003 (minimum value found for x = 0.4)\n",
      "* Co2FeGa0.5Ge0.5: Gilbert damping parameter not specified in the article\n",
      "\n",
      "Note that the article mentions that the Gilbert damping constant is proportional to the square of the spin-orbit coupling parameter and total density of states of the d-band at Fermi energy, but does not provide\n",
      "\n",
      "\n",
      "The Gilbert damping constant of Mn 3Sn is given as α = 0.05 K in the text.\n",
      "\n",
      "The material mentioned in the text is Mn 3Sn, and its Gilbert damping constant is listed as α = 0.02 (1.0) in the text.\n",
      "\n",
      "* Material: Mn 3Sn\n",
      "* Gilbert damping constant: α = 0.0005\n",
      "\n",
      "The Gilbert damping constant of the material mentioned in the text is Py (permalloy). The corresponding value of the Gilbert damping constant is 0.025 rad/s.\n",
      "\n",
      "The Gilbert damping constant of the material mentioned in the text is:\n",
      "\n",
      "* MgO: 0.008\n",
      "\n",
      "This value was obtained from the experimental data presented in Figure 1 of the paper.\n",
      "\n",
      "The Gilbert damping constant mentioned in the text is related to the dynamic spin susceptibility of the superconductors (SCs) and is given by:\n",
      "\n",
      "Gilbert damping constant = 2SJ21DF/(NkBTc)\n",
      "\n",
      "Where:\n",
      "\n",
      "* S is the surface area of the SC\n",
      "* J2 is the interface coupling strength\n",
      "* DF is the density of states at the Fermi level in the normal state\n",
      "* N is the number of units of the SC\n",
      "* k is the wave vector\n",
      "* B is the magnetic field\n",
      "*\n",
      "\n",
      "The material mentioned in the article with a low Gilbert damping constant is YIG (Yttrium Iron Garnet). The Gilbert damping constant of YIG is approximately 0.01 rad/s.\n",
      "\n",
      "The material mentioned in the text is Yttrium Iron Garnet (YIG). The Gilbert damping constant of YIG is α = 6.7 × 10^-5.\n",
      "\n",
      "The Gilbert damping constant mentioned in the text is:\n",
      "\n",
      "* For the YIG film at room temperature (295 K): $K_g = 0.02$ m/s\n",
      "* For the YIG film at 5 K: $K_g = 0.008$ m/s\n",
      "\n",
      "These values are obtained by fitting the experimental data to the LLG equation model described in the text.\n",
      "\n",
      "\n",
      "The Gilbert damping constant of the material mentioned in the text is:\n",
      "\n",
      "* Nb: α = 0.012 at T = 10 K, α = 0.0054 at T = 4 K\n",
      "* NiFe: not mentioned in the text\n",
      "\n",
      "Note that the Gilbert damping constant is a measure of the energy loss of the magnetic moments due to their interaction with the surrounding electromagnetic fields, and it is an important parameter in understanding the magnetic dynamics of materials.\n",
      "\n",
      "The material mentioned in the text is Yttrium Iron Garnet (YIG). The Gilbert damping constant of YIG is approximately 10^-3.\n",
      "\n",
      "\n",
      "* Material: YIG\n",
      "* Gilbert damping constant: 𝛾 = 0.46 rad/ns·T\n",
      "* Inhomogeneous linewidth broadening at zero field: 𝜇0∆𝐻(0) = 0.5 rad\n",
      "* Anisotropy field: 𝐻c = 1.77 T\n",
      "* Magnetization at saturation: 𝑀s = 2.4 x 10^5 A/m\n",
      "\n",
      "For the Ga:YIG films, the Gilbert d\n",
      "\n",
      "The Gilbert damping constant mentioned in the text is α = 0.01.\n",
      "\n",
      "The Gilbert damping constant of the material mentioned in the text is α = 0.05.\n",
      "\n",
      "The Gilbert damping constant (α) for the material Py is listed as approximately 1.83 x 10^11 rad/s^(-1)T^(-1) in the text.\n",
      "\n",
      "The material mentioned in the text is:\n",
      "\n",
      "* Iron (Fe)\n",
      "\n",
      "The Gilbert damping constant (α) for iron is not explicitly mentioned in the text, but it can be estimated based on the materials properties. The Gilbert damping constant is related to the viscosity of the material and can be calculated using the following equation:\n",
      "\n",
      "α = (H / ω_0^2) \\* (1 / 2π)\n",
      "\n",
      "where H is the magnetic field strength, ω_0 is the resonance frequency, and the factor (1 / 2π) accounts for the\n",
      "\n",
      "The Gilbert damping constant of the material mentioned in the text is:\n",
      "\n",
      "* NiO: \u000b",
      "= 0.00854 meV\n",
      "\n",
      "This value is obtained from the parameter estimates given in the text, specifically from Ref. [25].\n",
      "\n",
      "The Gilbert damping constant mentioned in the text is α = 0.023.\n",
      "\n",
      "The Gilbert damping constant of the material used in the simulation is K = 2.3 kJ/m3.\n",
      "\n",
      "The Gilbert damping constant of a certain material can be calculated using the formula provided in the reference text. The Gilbert damping constant, α, is related to the imaginary part of the self-energy of the magnon Green's function, ImΣR(0, ω), and is given by:\n",
      "\n",
      "α = 2S ℏω ImΣR(0, ω) (56)\n",
      "\n",
      "where S is the spin number of the Majorana mode, ℏ is the Planck constant divided by 2π, and ω is the angular frequency of\n",
      "\n",
      "* Material: Mn3Sn\n",
      "* Gilbert damping constant: α = 0.003\n",
      "\n",
      "In the paper, the authors discuss the evaluation of the Gilbert damping constant in magnetic insulators using first-principles based approaches. They apply their method to three materials: Y3Fe5O12, MnFe2O4, and Cr2O3. The Gilbert damping constant was calculated to be 0.8 x 10^-4, 0.2 x 10^-4, and 2.2 x 10^-4, respectively, at a low temperature. The results for Y3Fe5O12 and Cr2O3 are in good\n",
      "\n",
      "The text mentions the Gilbert damping constant of several materials, including:\n",
      "\n",
      "* YIG (Yttrium Iron Garnet): The microscopic origin of low damping in YIG will be investigated in the work.\n",
      "* Transition metals: The authors of the text mention that their previous works on magnetic insulators have resulted in estimates of the Gilbert damping constant that are both orders of magnitude smaller than experimental values, likely due to the use of empirically parameterized models.\n",
      "\n",
      "Based on the given text, the following materials and their corresponding Gilbert damping constants can be identified:\n",
      "\n",
      "* Y3Fe5O12: γ = 0.056 ps^(-1) [32]\n",
      "* MnFe2O4: γ = 0.036 ps^(-1) [32]\n",
      "* Cr2O3: γ = 0.06 ps^(-1) [32]\n",
      "\n",
      "Note that these values are mentioned in the context of evaluating the damping constant through spin-lattice dynamics simulations\n",
      "\n",
      "The Gilbert damping constant of YIG (Yttrium Iron Garnet) is not explicitly mentioned in the given text. However, the text does provide some information related to the damping constant of YIG.\n",
      "\n",
      "According to the text, the unique property of YIG is its ultra-low magnetic damping, which has attracted significant attention in the field of spintronics. To investigate the intrinsic mechanism behind this property, the authors of the paper simulate the relaxation process of the spin system in YIG using the Nonequilibrium Green's Function (NEGF) method\n",
      "\n",
      "The material mentioned in the passage is YIG (Yttrium Iron Garnet). The Gilbert damping constant of YIG is listed as follows:\n",
      "\n",
      "* At T = 0 K: α = 3.2 × 10^-4 [Ref. 47]\n",
      "* At T = 15 K: α = 2.8 ± 0.3 × 10^-5 [Experimental measurement]\n",
      "* At T = 30 K: α = 7.0 ± 0.7 × 10^-5 [Experimental measurement]\n",
      "\n",
      "\n",
      "\n",
      "The material mentioned in the text is Cadmium Mercury Telluride (CMG).\n",
      "\n",
      "The Gilbert damping constant of CMG is:\n",
      "\n",
      "α = 0.025 ± 0.001 T for (001) plane\n",
      "α = 0.014 ± 0.001 T for (110) plane\n",
      "\n",
      "The material mentioned in the text is Co2MnGa, and its Gilbert damping constant is α = 0.018 ± 0.001.\n",
      "\n",
      "The Gilbert damping constant of the material is not mentioned in the provided text. However, based on the context, it seems that the Gilbert damping constant of the material is α = 0.04. This value is used in the numerical calculations throughout the article.\n",
      "\n",
      "Here are the materials and their corresponding Gilbert damping constants mentioned in the text:\n",
      "\n",
      "* SyF (Shunt-Fabry-Perot) - The Gilbert damping constant for this material is not specified in the text.\n",
      "* SyF-pinned (Shunt-Fabry-Perot with pinned magnetization) - The Gilbert damping constant for this material is not specified in the text.\n",
      "* SyF-FL (Shunt-Fabry-Perot with free layer) - The Gilbert damping constant for this material is α1 = 0.\n",
      "\n",
      "The Gilbert damping constant of the materials mentioned in the text are:\n",
      "\n",
      "* CFB thin film: αeff = 0.0059 ±0.0001\n",
      "* CFW series: αeff = 0.0075 ±0.0001 (for CFW1) and αeff = 0.0080 ±0.0001 (for CFW2)\n",
      "* CFWC series: αeff = 0.0065 ±0.0001\n",
      "\n",
      "In this article, the Gilbert damping constant of several materials is discussed. The materials mentioned are:\n",
      "\n",
      "* Bcc iron (Fe)\n",
      "* Fcc cobalt (Co)\n",
      "* (001)-oriented surfaces of Fe and Co\n",
      "\n",
      "The Gilbert damping constant is determined using an ab initio scheme based on linear response theory of exchange torque correlation, implemented in the real-space Korringa-Kohn-Rostoker (RS-KKR) framework. The method is applied to these materials and the effects of lattice compression and surface relaxation on the Gilbert damp\n",
      "\n",
      "The Gilbert damping constant (α) for the materials mentioned in the text are:\n",
      "\n",
      "* Ir buffered FePd: α = 0.018\n",
      "* Cr/Ru buffered FePd: α = 0.008\n",
      "* Cr/Rh buffered FePd: α = 0.012\n",
      "* Cr/Pt buffered FePd: α = 0.004\n",
      "\n",
      "Note that these values are extracted from the fit of the frequency-dependent FMR field data to the linear expression in Equation (1) and are\n",
      "\n",
      "The material mentioned in the paper is FeRh, and its Gilbert damping constant is found to decrease with increasing temperature. The specific value of the Gilbert damping constant for FeRh is not provided in the paper.\n",
      "\n",
      "The Gilbert damping constant of the material mentioned in the text is FeRh. The value of the Gilbert damping constant is not explicitly stated in the text, but it can be inferred from the temperature dependence of the magnetization measured in the FMR experiments. According to the text, the effective Gilbert damping constant (αeff) decreases with increasing temperature, indicating that the temperature variation of the effective Gilbert damping constant reflects the evolution of the interface between the AFM and FM phases in the film.\n",
      "\n",
      "The material mentioned in the text is FeRh, and its Gilbert damping constant is listed as αeff=0.025 at 370 K.\n",
      "\n",
      "The material mentioned in the text is FeRh, and its Gilbert damping constant is (8.6±0.7)×10^-2 at 420 K.\n",
      "\n",
      "\n",
      "* BST/CoFeB: Gilbert damping constant = 0.0075\n",
      "* BST/Ti(0.5)/CoFeB: Gilbert damping constant = 0.0063\n",
      "* BST/Ru(0.5)/CoFeB: Gilbert damping constant = 0.0058\n",
      "\n",
      "The Gilbert damping constant of the material mentioned in the text is:\n",
      "\n",
      "* PMA CoFeB thin film: α = 6.1 kOe\n",
      "\n",
      "This information is found in Figure 3d of the text, which shows the dependence of the damping constant (α) on the test temperature (Ttest) for as-deposited and 400 °C-annealed FePd films.\n",
      "\n",
      "The Gilbert damping constant of the material mentioned in the article is:\n",
      "\n",
      "* Fe: α = 0.026 erg/cm3 (at room temperature)\n",
      "* Ge: α = 0.006 erg/cm3 (at room temperature)\n",
      "\n",
      "These values were obtained through ferromagnetic resonance (FMR) measurements.\n",
      "\n",
      "The Gilbert damping constant of the material mentioned in the text is:\n",
      "\n",
      "* For pristine S1: α ~ 0.0036 – 0.0085\n",
      "* For annealed S2: α ~ 0.021\n",
      "* For annealed S2: α ~ 0.012\n",
      "\n",
      "Note that these values are based on the text and may not be directly applicable to all materials or experimental conditions.\n",
      "\n",
      "The material mentioned in the passage is Co2MnSi, and its Gilbert damping constant is listed as avalue of approximately 0.0035.\n",
      "\n",
      "The Gilbert damping constant (α) of some magnetic garnet materials are:\n",
      "\n",
      "* Yttrium Iron Garnet (YIG): α = 3 × 10^-5\n",
      "* Barium Hexagonal Ferrite (BaFe12O19): α = 10^-4 - 10^-3\n",
      "\n",
      "Note that the value of α can vary depending on the specific material and growth conditions.\n",
      "\n",
      "\n",
      "* Material: YIG (Yttrium Iron Garnet)\n",
      "* Gilbert damping constant: 2829 MHz/kOe (for the YIG film) and 2807 MHz/kOe (for the YIG/Bi2Se3 film)\n",
      "\n",
      "Note: These values are mentioned in Figure 19(a) and (f) of the reference.\n",
      "\n",
      "The material mentioned in the article is [Co/Ni] n multilayers. The Gilbert damping constant of this material is listed as α = 0.027 meV/cm.\n",
      "\n",
      "The Gilbert damping constant of the materials mentioned in the text are:\n",
      "\n",
      "* Pt: 0.02-0.04\n",
      "* Co: 0.05-0.1\n",
      "* Ni: 0.02-0.04\n",
      "\n",
      "These values are based on the assumption that the materials have a fcc (111) texture, which is a necessary condition for the existence of the PMA.\n",
      "\n",
      "\n",
      "The Gilbert damping constant of the materials mentioned in the article can be found in the following table:\n",
      "\n",
      "| Material | Gilbert Damping Constant |\n",
      "| --- | --- |\n",
      "| Pt(5)/[Co/Ni] 6 | 0.037 |\n",
      "| Pt(5)/[Co/Ni] 8 | 0.027 |\n",
      "| Pt(5)/[Co/Ni] 10 | 0.023 |\n",
      "\n",
      "These values were extracted from the figure captions and are given for the specific sample\n",
      "\n",
      "The Gilbert damping constant (α) mentioned in the article is:\n",
      "\n",
      "* Gilbert damping constant (α) = 0.05\n",
      "\n",
      "This value of α is used in the LLG equation to describe the magnetization dynamics of the FM spins in the magnetic thin film system.\n",
      "\n",
      "\n",
      "Based on the text, the Gilbert damping constant of Y3Fe5O12 (YIG) is:\n",
      "\n",
      "α = 0.08 μV/K (for Re-YIG materials films)\n",
      "α = 0.065 μV/K (for single crystalline YIG thin films)\n",
      "α = 0.02 μV/K (for YIG films prepared by co-precipitation method)\n",
      "\n",
      "Note that these values are mentioned in the context of the study of the changes in the longitudinal spin Seebe\n",
      "\n",
      "* Material: Yttrium Iron Garnet (YIG)\n",
      "* Gilbert damping constant: α = 0.025 emu/cm3 (for YIG films)\n",
      "* Gilbert damping constant: α = 0.02 emu/cm3 (for Pt/YIG films)\n",
      "\n",
      "Based on the given text, the Gilbert damping constant of YIG (Yttrium Iron Garnet) is:\n",
      "\n",
      "* Without Pt layer: α = 0.035 cm²/s\n",
      "* With Pt layer (5 nm): α = 0.025 cm²/s\n",
      "\n",
      "These values are mentioned in Figure 4(b) and Figure 5(g) of the text.\n",
      "\n",
      "Material: FeGe\n",
      "Gilbert damping constant: α = 0.8\n",
      "\n",
      "The materials mentioned in the article are:\n",
      "\n",
      "* CoFeB (Copper-Ferrite-Barium)\n",
      "* Pt (Platinum)\n",
      "* V (Vanadium)\n",
      "* Zr (Zirconium)\n",
      "* Ta (Tantalum)\n",
      "\n",
      "The Gilbert damping constant for these materials are not explicitly mentioned in the article, but they can be estimated based on the literature values. Here are some approximate values of the Gilbert damping constant for these materials:\n",
      "\n",
      "* CoFeB: α = 0.02 - 0.04\n",
      "\n",
      "The material mentioned in the passage is Yttrium Iron Garnet (YIG). The Gilbert damping constant of YIG is not explicitly mentioned in the passage, but it can be calculated based on the data provided.\n",
      "\n",
      "Using the equation given in the passage:\n",
      "\n",
      "ΔH0 = α \\* f^2\n",
      "\n",
      "where α is the intrinsic Gilbert damping parameter, and f is the resonance frequency, we can calculate the Gilbert damping constant for the 430 nm thick YIG film.\n",
      "\n",
      "Assuming that the resonance frequency is given by:\n",
      "\n",
      "f =\n",
      "\n",
      "The Gilbert damping constant (α) of the materials mentioned in the text are:\n",
      "\n",
      "* FeCl2: α = 7.9 × 10^-5 esu\n",
      "* FeBr2: α = 1.6 × 10^-4 esu\n",
      "* FeI2: α = 3.7 × 10^-4 esu\n",
      "\n",
      "Note: esu stands for \"electronvolts per second\".\n",
      "\n",
      "The Gilbert damping constant for CoFeB is α = 0.025 rad/s.\n",
      "\n",
      "The Gilbert damping constant of D03-Fe 3Ga is (6.0±0.2)×10^-3.\n",
      "\n",
      "The Gilbert damping constant of the material is listed in the table below:\n",
      "\n",
      "| Material | Gilbert Damping Constant |\n",
      "| --- | --- |\n",
      "| Fe | 2.07 x 10^-32 J/m^3 |\n",
      "| Fe3Ga | 6.9 x 10^-32 J/m^3 |\n",
      "\n",
      "Note that the values are given in units of J/m^3, which is the SI unit of magnetic susceptibility.\n",
      "\n",
      "Here are the materials and their corresponding Gilbert damping constants mentioned in the text:\n",
      "\n",
      "* D03-Fe 3Ga: α = (6.0 ± 0.2) × 10^-3\n",
      "* Fe: α = (2.3 ± 0.2) × 10^-3\n",
      "\n",
      "Note that the Gilbert damping constant is a measure of the energy loss of a magnetization due to internal magnetic fluctuations, and it is typically expressed in units of energy (e.g., electronvolts) per unit time (e.g.,\n"
     ]
    }
   ],
   "source": [
    "for i in data:\n",
    "    input_prompt = generate_prompt(i['content'])\n",
    "    input_tokens = tokenizer(input_prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "    with torch.cuda.amp.autocast():\n",
    "        generation_output = model.generate(\n",
    "            input_ids=input_tokens,\n",
    "            max_new_tokens=128,\n",
    "            do_sample=True,\n",
    "            top_k=5,\n",
    "            top_p=0.9,\n",
    "            temperature=0.2,\n",
    "            repetition_penalty=1.1,\n",
    "            num_return_sequences=1,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    op = tokenizer.decode(generation_output[0], skip_special_tokens=True)\n",
    "    #print(op)\n",
    "    \n",
    "    inst_index = op.find('[/INST]')\n",
    "    \n",
    "    if inst_index != -1:\n",
    "        print(op[inst_index + len('[/INST]'):])\n",
    "    else:\n",
    "        print(\"未找到'[/INST]'标记\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d878c63e-2a64-4790-a3a0-5ad01d25706e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37553"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(probabilities[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c6db7b0b-f14a-4c1c-8cc5-765ffeca6795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,   673,   278, 17279, 29901,    13,  3561,  1212,   293, 17279,\n",
       "           411,  4482, 15611,   270,  1160,   292,  4868, 29889,    13,   450,\n",
       "         13206, 16637,  7063,   310,   278,  5518, 29901]], device='cuda:0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ae5a43c-8612-44c3-8b8a-ddc8e679b73d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,   673,   278, 17279, 29901,    13,  3561,  1212,   293, 17279,\n",
       "           411,  4482, 15611,   270,  1160,   292,  4868, 29889,    13,   450,\n",
       "         13206, 16637,  7063,   310,   278,  5518, 29901, 37551]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4dda011c-9377-4412-af00-4297571242c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('result', save_embedding_layers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601970e5-872e-4680-8244-01c41a211fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    print(param.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "291eb404-d313-4f1f-87c3-e489d24d60e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(37553, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=256, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=256, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=256, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=256, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "              (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "              (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm()\n",
       "            (post_attention_layernorm): LlamaRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=37553, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e506f375-179c-4782-a1c7-44079a380f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False base_model.model.model.embed_tokens.base_layer.weight torch.float16\n",
      "True base_model.model.model.embed_tokens.lora_embedding_A.default torch.float16\n",
      "True base_model.model.model.embed_tokens.lora_embedding_B.default torch.float16\n",
      "False base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.0.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.0.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.0.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.0.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.0.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.1.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.1.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.1.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.1.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.1.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.1.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.1.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.2.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.2.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.2.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.2.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.2.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.2.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.2.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.3.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.3.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.3.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.3.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.3.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.3.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.3.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.4.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.4.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.4.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.4.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.4.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.4.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.4.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.5.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.5.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.5.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.5.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.5.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.5.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.5.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.6.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.6.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.6.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.6.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.6.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.6.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.6.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.7.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.7.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.7.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.7.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.7.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.7.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.7.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.8.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.8.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.8.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.8.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.8.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.8.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.8.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.9.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.9.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.9.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.9.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.9.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.9.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.9.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.10.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.10.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.10.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.10.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.10.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.10.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.10.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.11.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.11.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.11.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.11.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.11.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.11.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.11.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.12.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.12.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.12.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.12.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.12.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.12.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.12.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.13.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.13.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.13.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.13.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.13.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.13.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.13.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.14.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.14.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.14.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.14.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.14.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.14.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.14.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.15.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.15.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.15.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.15.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.15.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.15.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.15.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.16.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.16.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.16.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.16.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.16.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.16.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.16.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.17.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.17.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.17.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.17.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.17.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.17.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.17.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.18.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.18.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.18.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.18.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.18.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.18.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.18.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.19.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.19.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.19.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.19.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.19.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.19.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.19.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.20.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.20.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.20.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.20.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.20.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.20.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.20.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.21.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.21.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.21.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.21.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.21.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.21.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.21.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.22.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.22.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.22.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.22.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.22.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.22.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.22.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.23.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.23.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.23.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.23.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.23.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.23.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.23.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.24.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.24.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.24.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.24.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.24.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.24.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.24.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.25.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.25.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.25.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.25.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.25.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.25.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.25.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.26.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.26.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.26.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.26.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.26.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.26.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.26.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.26.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.26.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.27.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.27.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.27.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.27.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.27.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.27.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.27.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.27.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.27.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.28.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.28.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.28.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.28.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.28.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.28.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.28.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.28.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.28.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.29.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.29.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.29.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.29.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.29.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.29.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.29.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.29.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.29.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.30.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.30.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.30.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.30.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.30.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.30.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.30.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.30.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.30.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.31.self_attn.q_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.31.self_attn.k_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.31.self_attn.v_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.31.self_attn.o_proj.base_layer.weight torch.float16\n",
      "True base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight torch.float16\n",
      "True base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight torch.float16\n",
      "False base_model.model.model.layers.31.mlp.gate_proj.weight torch.float16\n",
      "False base_model.model.model.layers.31.mlp.up_proj.weight torch.float16\n",
      "False base_model.model.model.layers.31.mlp.down_proj.weight torch.float16\n",
      "False base_model.model.model.layers.31.input_layernorm.weight torch.float16\n",
      "False base_model.model.model.layers.31.post_attention_layernorm.weight torch.float16\n",
      "False base_model.model.model.norm.weight torch.float16\n",
      "False base_model.model.lm_head.weight torch.float16\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(param.requires_grad, name, param.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81fa8868-d7b2-4b52-9c5d-8d46a3d0af2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable: base_model.model.model.embed_tokens.weight\n",
      "Trainable: base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight\n",
      "Trainable: base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight\n",
      "Trainable: base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight\n"
     ]
    }
   ],
   "source": [
    "# Verify which parameters are trainable\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"Trainable: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346aa5df-43be-459d-98fc-247b0a2fda7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fa5b3e-125c-4fb1-b073-82301ba20d66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53006efd-ca7f-4cf7-b226-331f6f4316b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"Trainable: {name}\", param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0de8f185-07da-46c0-a3af-0f45f6b39016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x7faf7c779ee0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0db0120f-b1a6-441a-8dbe-d0e59fe4e9f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(38544, 4096)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_input_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a34e1d06-cd59-4b28-9915-a865bb031c2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6922694656"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "50d8bd59-e9dd-4a70-9e38-02944052fd9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(38545, 4096)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({\"pad_token\": \"<PAD>\"})\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621f10f6-db0d-47ad-9c4f-6d4cc888b013",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
